{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 1: Open all files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "anime_train = pd.read_csv(\".//Database//Anime_train.csv\")#.head(500)\n",
    "anime_test = pd.read_csv(\".//Database//Anime_test.csv\")#.head(10)\n",
    "\n",
    "char_train = pd.read_csv(\".//Database//Char_train.csv\")\n",
    "char_test = pd.read_csv(\".//Database//Char_test.csv\")\n",
    "\n",
    "impath_train = \".//Database//char train//\"\n",
    "impath_test = \".//Database//char test//\"\n",
    "\n",
    "\n",
    "#Original format for characters:\n",
    "anime_train[\"Main Characters\"] = anime_train[\"Main Characters\"].apply(lambda x:[int(y) for y in x[1:-1].split(\", \")])\n",
    "anime_test[\"Main Characters\"] = anime_test[\"Main Characters\"].apply(lambda x:[int(y) for y in x[1:-1].split(\", \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ALL GENERAL VALUES:\n",
    "\n",
    "batch_size = 16\n",
    "max_length_syn = 128\n",
    "max_length_char = 256\n",
    "epochs = 5 #1000\n",
    "\n",
    "learning_rate = 5e-5\n",
    "eps_value = 1e-8\n",
    "syn_model_name_or_path = 'gpt2'\n",
    "char_model_name_or_path = 'gpt2'\n",
    "img_model_name_or_path = 'microsoft/resnet-50'\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"torch running in {device}\")\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 2: Open all datasets\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scaler(values, ratio): #Only works with positive values.\n",
    "    return [(v - min(values)) * (ratio / (max(values) - min(values))) for v in values]\n",
    "\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, df, char_df, mchars, img_path, transform=None):\n",
    "\n",
    "        self.labels = []\n",
    "        for x in df.index:\n",
    "            self.labels.append(df[\"Score\"][x])\n",
    "        \n",
    "        self.syn = []\n",
    "        for x in df.index:\n",
    "            self.syn.append(df[\"Synopsis\"][x].split(\"(source\")[0].split(\"(Source\")[0].split(\"[Written\")[0])\n",
    "\n",
    "        self.mchars = []\n",
    "        for x in df.index:\n",
    "            list_chars = mchars[x]\n",
    "            chars = []\n",
    "            for y in list_chars:\n",
    "                personaje = char_df[char_df[\"MAL ID\"] == y].reset_index()\n",
    "                chars.append(personaje[\"Description\"][0].split(\"(source\")[0].split(\"(Source\")[0].split(\"[Written\")[0])\n",
    "            self.mchars.append(' '.join(chars))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        for x in tqdm(df.index, desc=\"Concatenating portraits\"):\n",
    "            list_chars = [str(y)+\".png\" for y in mchars[x]]\n",
    "            chars = []\n",
    "            for y in list_chars:\n",
    "                img_name = img_path+y\n",
    "                personaje = Image.open(img_name).convert('RGB') #Abrir la imagen\n",
    "                chars.append(personaje)\n",
    "\n",
    "            #Appending all portraits, horizontally.\n",
    "            widths, heights = zip(*(img.size for img in chars))\n",
    "            retratos = Image.new('RGB', (sum(widths), max(heights)))\n",
    "            \n",
    "            x_offset = 0\n",
    "            for img in chars:\n",
    "                retratos.paste(img, (x_offset, 0))\n",
    "                x_offset += img.width\n",
    "\n",
    "            if self.transform:\n",
    "                retratos = self.transform(retratos)\n",
    "\n",
    "            self.img.append(retratos)\n",
    "\n",
    "        self.labels = scaler(self.labels, 1)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'synopsis':self.syn[item], 'mchar':self.mchars[item], 'img':self.img[item], 'label':self.labels[item]}\n",
    "    \n",
    "\n",
    "transf = None\n",
    "####It can be:\n",
    "##\n",
    "##transf = transforms.Compose([\n",
    "##    transforms.Resize(size),        # Resize images to a fixed size\n",
    "##    transforms.ToTensor(),          # Convert images to tensors\n",
    "##    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "##    ])\n",
    "\n",
    "train_dataset = AnimeDataset(df=anime_train, char_df=char_train, mchars=anime_train[\"Main Characters\"], img_path=impath_train, transform=transf)\n",
    "test_dataset = AnimeDataset(df=anime_test, char_df=char_test, mchars=anime_test[\"Main Characters\"], img_path=impath_test, transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 3: Collate all the data into a unified input\n",
    "\n",
    "class AnimeRegressionCollator(object):\n",
    "    def __init__(self, img_processor, syn_tokenizer, char_tokenizer, syn_max_sequence_len=None, char_max_sequence_len=None):\n",
    "        self.syn_tokenizer = syn_tokenizer\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.syn_max_sequence_len = syn_tokenizer.model_max_length if syn_max_sequence_len is None else syn_max_sequence_len\n",
    "        self.char_max_sequence_len = char_tokenizer.model_max_length if char_max_sequence_len is None else char_max_sequence_len\n",
    "        self.img_processor = img_processor\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "\n",
    "        synopsis = [sequence['synopsis'] for sequence in sequences]\n",
    "        char = [sequence['mchar'] for sequence in sequences]\n",
    "        img = [sequence['img'] for sequence in sequences]\n",
    "\n",
    "        inputs = {'portraits': self.img_processor(images=img, return_tensors=\"pt\")}\n",
    "        inputs.update({'synopsis': self.syn_tokenizer(text=synopsis, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.syn_max_sequence_len)})\n",
    "        inputs.update({'descriptions': self.char_tokenizer(text=char, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.char_max_sequence_len)})\n",
    "        inputs.update({'labels': torch.tensor(np.array(labels), dtype=torch.float)})\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "syn_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=syn_model_name_or_path)\n",
    "syn_tokenizer.padding_side = \"left\" # default to left padding\n",
    "syn_tokenizer.pad_token = syn_tokenizer.eos_token # Define PAD Token = EOS Token = 50256\n",
    "\n",
    "char_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=char_model_name_or_path)\n",
    "char_tokenizer.padding_side = \"left\" # default to left padding\n",
    "char_tokenizer.pad_token = char_tokenizer.eos_token # Define PAD Token = EOS Token = 50256\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name_or_path=img_model_name_or_path)\n",
    "\n",
    "regression_collator = AnimeRegressionCollator(img_processor=processor,\n",
    "                                            syn_tokenizer=syn_tokenizer, \n",
    "                                            char_tokenizer=char_tokenizer, \n",
    "                                            syn_max_sequence_len=max_length_syn, \n",
    "                                            char_max_sequence_len=max_length_char)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=regression_collator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=regression_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 4: Initialize the neural network\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, ResNetModel\n",
    "\n",
    "class AniNet(nn.Module):\n",
    "    def __init__(self, pretr_a, pretr_b, pretr_img): #img_input_size\n",
    "        super(AniNet, self).__init__()\n",
    "        #self.img_input_size = img_input_size\n",
    "\n",
    "        #CHARACTERS:\n",
    "        self.img_net = ResNetModel.from_pretrained(pretr_img)\n",
    "        self.model_b = GPT2Model.from_pretrained(pretr_b)\n",
    "\n",
    "        self.char_classifier = nn.Sequential(\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.Linear(768+49, 768, bias=True),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.Linear(768, 768, bias=True)\n",
    "                )\n",
    "        \n",
    "        #SYNOPSIS:\n",
    "        self.model_a = GPT2Model.from_pretrained(pretr_a)\n",
    "\n",
    "        #ALL:\n",
    "        self.final_classifier = nn.Sequential(\n",
    "                nn.Linear(2*768,768),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(768, 384),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(384, 192),\n",
    "                nn.Tanh(),\n",
    "                #nn.Dropout(p=0.1),\n",
    "                nn.Linear(192, 96),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(96, 48),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(48, 24),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(24, 12),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(12, 6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(6, 1)\n",
    "                )\n",
    "        \n",
    "    def forward(self, syn_input_ids, char_input_ids, syn_attention_mask, char_attention_mask, img_input_ids):\n",
    "\n",
    "        #Synopsis:\n",
    "        logits_a = self.model_a(syn_input_ids, attention_mask=syn_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "        #Characters:\n",
    "        logits_b = self.model_b(char_input_ids, attention_mask=char_attention_mask).last_hidden_state[:, 0, :]\n",
    "        img_output = self.img_net(img_input_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "        img_output = img_output.view(img_output.shape[0], -1) #Flattening to shape [bsize, 49]\n",
    "\n",
    "        char = torch.concat((logits_b, img_output), dim=1)\n",
    "        char = self.char_classifier(char)\n",
    "\n",
    "        #ALL:\n",
    "        concatenated_vectors = torch.concat((logits_a, char), dim=1)\n",
    "        output = self.final_classifier(concatenated_vectors)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = AniNet(pretr_a=syn_model_name_or_path, pretr_b=char_model_name_or_path, pretr_img=img_model_name_or_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, predictions, true_labels, optimizer_, scheduler_, device_, loss_fn):\n",
    "    global model\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, total=len(train_loader), desc=\"Batch\"):\n",
    "\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        ##INPUTS:\n",
    "        syn_input_ids = batch['synopsis']['input_ids'].type(torch.long).to(device_)\n",
    "        syn_attention_mask = batch['synopsis']['attention_mask'].type(torch.long).to(device_)\n",
    "\n",
    "        char_input_ids = batch['descriptions']['input_ids'].type(torch.long).to(device_)\n",
    "        char_attention_mask = batch['descriptions']['attention_mask'].type(torch.long).to(device_)\n",
    "\n",
    "        img_key = 'pixel_values' #'input_ids'\n",
    "        img_input_ids = batch['portraits'][img_key].type(torch.float).to(device_)\n",
    "\n",
    "        \n",
    "        outputs = model(syn_input_ids=syn_input_ids, char_input_ids=char_input_ids, syn_attention_mask=syn_attention_mask, char_attention_mask=char_attention_mask, img_input_ids=img_input_ids).to(device_)\n",
    "        \n",
    "        logits = outputs\n",
    "\n",
    "        predictions_loss = logits.squeeze()\n",
    "\n",
    "        lbels = torch.Tensor(batch['labels'].float()).to(device_)\n",
    "        loss = loss_fn(predictions_loss, lbels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #optimizer_.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer_.step()\n",
    "        scheduler_.step()\n",
    "\n",
    "        predictions += predictions_loss\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    return true_labels, predictions, avg_epoch_loss\n",
    "\n",
    "\n",
    "def validation(test_loader, device_, loss_fn):\n",
    "    global model\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        \n",
    "        ##INPUTS:\n",
    "        syn_input_ids = batch['synopsis']['input_ids'].type(torch.long).to(device_)\n",
    "        syn_attention_mask = batch['synopsis']['attention_mask'].type(torch.long).to(device_)\n",
    "\n",
    "        char_input_ids = batch['descriptions']['input_ids'].type(torch.long).to(device_)\n",
    "        char_attention_mask = batch['descriptions']['attention_mask'].type(torch.long).to(device_)\n",
    "\n",
    "        img_key = 'pixel_values' #'input_ids'\n",
    "        img_input_ids = batch['portraits'][img_key].type(torch.float).to(device_)\n",
    "\n",
    "\n",
    "        with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            outputs = model(syn_input_ids=syn_input_ids, char_input_ids=char_input_ids, syn_attention_mask=syn_attention_mask, char_attention_mask=char_attention_mask, img_input_ids=img_input_ids).to(device_)\n",
    "            logits = outputs\n",
    "\n",
    "            predictions += logits.squeeze().detach().cpu().tolist()\n",
    "            predictions_loss = torch.Tensor(logits.squeeze().detach().cpu()).to(device_)\n",
    "\n",
    "            loss = loss_fn(predictions_loss, torch.Tensor(batch['labels'].float()).to(device_))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(test_loader)\n",
    "\n",
    "    return true_labels, predictions, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ca_Naxca import regression_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "##TRAIN THE MODEL.\n",
    "\n",
    "optimizer_ = torch.optim.AdamW(model.parameters(), lr = learning_rate, eps = eps_value)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler_ = get_linear_schedule_with_warmup(optimizer_, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "loss_fn = nn.MSELoss()  # Loss function for regression problems\n",
    "\n",
    "\n",
    "trainval = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    avg_epoch_loss = 0\n",
    "\n",
    "    # Train the model:\n",
    "    true_labels, predictions, train_loss = training_loop(train_dataloader, predictions, true_labels, optimizer_, scheduler_, device, loss_fn)\n",
    "    # Test the model:\n",
    "    valid_labels, valid_predict, val_loss = validation(test_dataloader, device, loss_fn)\n",
    "\n",
    "    # Is it good enough?\n",
    "    reporte = regression_report(valid_labels, valid_predict, [i for i in range(len(valid_labels))])\n",
    "    reps = reporte.display()\n",
    "    print(reps)\n",
    "    reps.to_csv(\".//final_reports//Full//reporte_full.csv\")\n",
    "\n",
    "    for x in [[valid_labels[i], valid_predict[i]] for i in range(10)]:\n",
    "        print(x)\n",
    "\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f \"%(train_loss, val_loss))\n",
    "    print()\n",
    "    trainval.append([train_loss, val_loss])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "jsonfile = open(\".//final_reports//Full//train_val_loss.json\", \"w\")\n",
    "json.dump(trainval, jsonfile)\n",
    "jsonfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
