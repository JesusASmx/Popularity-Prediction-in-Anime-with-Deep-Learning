{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 1: Open all files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "anime_train = pd.read_csv(\".//Database//Anime_train.csv\")\n",
    "anime_test = pd.read_csv(\".//Database//Anime_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ALL GENERAL VALUES:\n",
    "\n",
    "batch_size = 16\n",
    "max_length_syn = 128\n",
    "max_length_char = 256\n",
    "epochs = 5 #1000\n",
    "\n",
    "learning_rate = 5e-5\n",
    "eps_value = 1e-8\n",
    "syn_model_name_or_path = 'gpt2'\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"torch running in {device}\")\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 2: Open all datasets\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scaler(values, ratio): #Only works with positive values.\n",
    "    return [(v - min(values)) * (ratio / (max(values) - min(values))) for v in values]\n",
    "\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "\n",
    "        self.labels = []\n",
    "        for x in df.index:\n",
    "            self.labels.append(df[\"Score\"][x])\n",
    "        \n",
    "        self.syn = []\n",
    "        for x in df.index:\n",
    "            self.syn.append(df[\"Synopsis\"][x].split(\"(source\")[0].split(\"(Source\")[0].split(\"[Written\")[0])\n",
    "\n",
    "        self.labels = scaler(self.labels, 1)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'synopsis':self.syn[item], 'label':self.labels[item]}\n",
    "    \n",
    "\n",
    "train_dataset = AnimeDataset(df=anime_train, transform=transf)\n",
    "test_dataset = AnimeDataset(df=anime_test, transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 3: Collate all the data into a unified input\n",
    "\n",
    "class AnimeRegressionCollator(object):\n",
    "    def __init__(self, syn_tokenizer, syn_max_sequence_len=None):\n",
    "        self.syn_tokenizer = syn_tokenizer\n",
    "        self.syn_max_sequence_len = syn_tokenizer.model_max_length if syn_max_sequence_len is None else syn_max_sequence_len\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "\n",
    "        synopsis = [sequence['synopsis'] for sequence in sequences]\n",
    "\n",
    "        inputs = {'synopsis': self.syn_tokenizer(text=synopsis, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.syn_max_sequence_len)}\n",
    "        inputs.update({'labels': torch.tensor(np.array(labels), dtype=torch.float)})\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "syn_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=syn_model_name_or_path)\n",
    "syn_tokenizer.padding_side = \"left\" # default to left padding\n",
    "syn_tokenizer.pad_token = syn_tokenizer.eos_token # Define PAD Token = EOS Token = 50256\n",
    "\n",
    "regression_collator = AnimeRegressionCollator(syn_tokenizer=syn_tokenizer, syn_max_sequence_len=max_length_syn)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=regression_collator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=regression_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 4: Initialize the neural network\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, ResNetModel\n",
    "\n",
    "class AniNet(nn.Module):\n",
    "    def __init__(self, pretr_a):\n",
    "        super(AniNet, self).__init__()\n",
    "\n",
    "        #SYNOPSIS:\n",
    "        self.model_a = GPT2Model.from_pretrained(pretr_a)\n",
    "\n",
    "        #ALL:\n",
    "        self.final_classifier = nn.Sequential(\n",
    "                nn.Linear(768, 384),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(384, 192),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(192, 96),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(96, 48),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(48, 24),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(24, 12),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(12, 6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(6, 1)\n",
    "                )\n",
    "        \n",
    "    def forward(self, syn_input_ids, syn_attention_mask):\n",
    "\n",
    "        #Synopsis:\n",
    "        logits_a = self.model_a(syn_input_ids, attention_mask=syn_attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "        #ALL:\n",
    "        output = self.final_classifier(logits_a)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = AniNet(pretr_a=syn_model_name_or_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, predictions, true_labels, optimizer_, scheduler_, device_, loss_fn):\n",
    "    global model\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, total=len(train_loader), desc=\"Batch\"):\n",
    "\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        ##INPUTS:\n",
    "        syn_input_ids = batch['synopsis']['input_ids'].type(torch.long).to(device_)\n",
    "        syn_attention_mask = batch['synopsis']['attention_mask'].type(torch.long).to(device_)\n",
    "        \n",
    "        outputs = model(syn_input_ids=syn_input_ids, syn_attention_mask=syn_attention_mask).to(device_)\n",
    "        \n",
    "        logits = outputs\n",
    "\n",
    "        predictions_loss = logits.squeeze()\n",
    "\n",
    "        lbels = torch.Tensor(batch['labels'].float()).to(device_)\n",
    "        loss = loss_fn(predictions_loss, lbels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer_.step()\n",
    "        scheduler_.step()\n",
    "\n",
    "        predictions += predictions_loss\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    return true_labels, predictions, avg_epoch_loss\n",
    "\n",
    "\n",
    "def validation(test_loader, device_, loss_fn):\n",
    "    global model\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        \n",
    "        ##INPUTS:\n",
    "        syn_input_ids = batch['synopsis']['input_ids'].type(torch.long).to(device_)\n",
    "        syn_attention_mask = batch['synopsis']['attention_mask'].type(torch.long).to(device_)\n",
    "\n",
    "        with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            outputs = model(syn_input_ids=syn_input_ids, syn_attention_mask=syn_attention_mask).to(device_)\n",
    "            logits = outputs\n",
    "\n",
    "            predictions += logits.squeeze().detach().cpu().tolist()\n",
    "            predictions_loss = torch.Tensor(logits.squeeze().detach().cpu()).to(device_)\n",
    "\n",
    "            loss = loss_fn(predictions_loss, torch.Tensor(batch['labels'].float()).to(device_))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(test_loader)\n",
    "\n",
    "    return true_labels, predictions, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ca_Naxca import regression_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "##TRAIN THE MODEL.\n",
    "\n",
    "optimizer_ = torch.optim.AdamW(model.parameters(), lr = learning_rate, eps = eps_value)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler_ = get_linear_schedule_with_warmup(optimizer_, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "loss_fn = nn.MSELoss()  # Loss function for regression problems\n",
    "\n",
    "\n",
    "trainval = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    avg_epoch_loss = 0\n",
    "\n",
    "    # Train the model:\n",
    "    true_labels, predictions, train_loss = training_loop(train_dataloader, predictions, true_labels, optimizer_, scheduler_, device, loss_fn)\n",
    "    # Test the model:\n",
    "    valid_labels, valid_predict, val_loss = validation(test_dataloader, device, loss_fn)\n",
    "\n",
    "    # Is it good enough?\n",
    "    reporte = regression_report(valid_labels, valid_predict, [i for i in range(len(valid_labels))])\n",
    "    reps = reporte.display()\n",
    "    print(reps)\n",
    "    reps.to_csv(\".//final_reports//Syn//reporte_syn.csv\")\n",
    "\n",
    "    for x in [[valid_labels[i], valid_predict[i]] for i in range(10)]:\n",
    "        print(x)\n",
    "\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f \"%(train_loss, val_loss))\n",
    "    print()\n",
    "    trainval.append([train_loss, val_loss])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "jsonfile = open(\".//final_reports//Syn//train_val_loss.json\", \"w\")\n",
    "json.dump(trainval, jsonfile)\n",
    "jsonfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
