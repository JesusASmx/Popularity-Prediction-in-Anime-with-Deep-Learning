{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 1: Open all files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "anime_train = pd.read_csv(\".//Database//Anime_train.csv\")\n",
    "anime_test = pd.read_csv(\".//Database//Anime_test.csv\")\n",
    "\n",
    "impath_train = \".//Database//char train//\"\n",
    "impath_test = \".//Database//char test//\"\n",
    "\n",
    "\n",
    "#Original format for characters:\n",
    "anime_train[\"Main Characters\"] = anime_train[\"Main Characters\"].apply(lambda x:[int(y) for y in x[1:-1].split(\", \")])\n",
    "anime_test[\"Main Characters\"] = anime_test[\"Main Characters\"].apply(lambda x:[int(y) for y in x[1:-1].split(\", \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ALL GENERAL VALUES:\n",
    "\n",
    "batch_size = 16\n",
    "max_length_syn = 128\n",
    "max_length_char = 256\n",
    "epochs = 5 #1000\n",
    "\n",
    "learning_rate = 5e-5\n",
    "eps_value = 1e-8\n",
    "syn_model_name_or_path = 'gpt2'\n",
    "img_model_name_or_path = 'microsoft/resnet-50'\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"torch running in {device}\")\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 2: Open all datasets\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scaler(values, ratio): #Only works with positive values.\n",
    "    return [(v - min(values)) * (ratio / (max(values) - min(values))) for v in values]\n",
    "\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, df, mchars, img_path, transform=None):\n",
    "\n",
    "        self.labels = []\n",
    "        for x in df.index:\n",
    "            self.labels.append(df[\"Score\"][x])\n",
    "\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        for x in tqdm(df.index, desc=\"Concatenating portraits\"):\n",
    "            list_chars = [str(y)+\".png\" for y in mchars[x]]\n",
    "            chars = []\n",
    "            for y in list_chars:\n",
    "                img_name = img_path+y\n",
    "                personaje = Image.open(img_name).convert('RGB') #Abrir la imagen\n",
    "                chars.append(personaje)\n",
    "\n",
    "            #Appending all portraits, horizontally.\n",
    "            widths, heights = zip(*(img.size for img in chars))\n",
    "            retratos = Image.new('RGB', (sum(widths), max(heights)))\n",
    "            \n",
    "            x_offset = 0\n",
    "            for img in chars:\n",
    "                retratos.paste(img, (x_offset, 0))\n",
    "                x_offset += img.width\n",
    "\n",
    "            if self.transform:\n",
    "                retratos = self.transform(retratos)\n",
    "\n",
    "            self.img.append(retratos)\n",
    "\n",
    "        self.labels = scaler(self.labels, 1)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'img':self.img[item], 'label':self.labels[item]}\n",
    "    \n",
    "\n",
    "transf = None\n",
    "####It can be:\n",
    "##\n",
    "##transf = transforms.Compose([\n",
    "##    transforms.Resize(size),        # You should first define a tuple \"size\". This can aid in memory saving.\n",
    "##    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images. Values are just illustrative.\n",
    "##    ])\n",
    "\n",
    "train_dataset = AnimeDataset(df=anime_train, mchars=anime_train[\"Main Characters\"], img_path=impath_train, transform=transf)\n",
    "test_dataset = AnimeDataset(df=anime_test, mchars=anime_test[\"Main Characters\"], img_path=impath_test, transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 3: Collate all the data into a unified input\n",
    "\n",
    "class AnimeRegressionCollator(object):\n",
    "    def __init__(self, img_processor):\n",
    "        self.img_processor = img_processor\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "\n",
    "        img = [sequence['img'] for sequence in sequences]\n",
    "\n",
    "        inputs = {'portraits': self.img_processor(images=img, return_tensors=\"pt\")}\n",
    "        inputs.update({'labels': torch.tensor(np.array(labels), dtype=torch.float)})\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name_or_path=img_model_name_or_path)\n",
    "\n",
    "regression_collator = AnimeRegressionCollator(img_processor=processor)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=regression_collator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=regression_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 4: Initialize the neural network\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import ResNetModel\n",
    "\n",
    "class AniNet(nn.Module):\n",
    "    def __init__(self, pretr_img):\n",
    "        super(AniNet, self).__init__()\n",
    "\n",
    "        #CHARACTERS:\n",
    "        self.img_net = ResNetModel.from_pretrained(pretr_img)\n",
    "\n",
    "        self.char_classifier = nn.Sequential(\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.Linear(49, 24, bias=True),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(24, 12, bias=True),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(12, 6, bias=True),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(6, 3, bias=True),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(3, 1, bias=True),\n",
    "                nn.ReLU(),\n",
    "                )\n",
    "        \n",
    "    def forward(self, img_input_ids):\n",
    "\n",
    "        img_output = self.img_net(img_input_ids).last_hidden_state[:, 0, :]\n",
    "        img_output = img_output.view(img_output.shape[0], -1) #Flattening to shape [bsize, 49]\n",
    "\n",
    "        output = self.char_classifier(img_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = AniNet(pretr_img=img_model_name_or_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, predictions, true_labels, optimizer_, scheduler_, device_, loss_fn):\n",
    "    global model\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, total=len(train_loader), desc=\"Batch\"):\n",
    "\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        ##INPUTS:\n",
    "        img_key = 'pixel_values' #'input_ids'\n",
    "        img_input_ids = batch['portraits'][img_key].type(torch.float).to(device_)\n",
    "\n",
    "        outputs = model(img_input_ids=img_input_ids).to(device_)\n",
    "        \n",
    "        logits = outputs\n",
    "\n",
    "        predictions_loss = logits.squeeze()\n",
    "\n",
    "        lbels = torch.Tensor(batch['labels'].float()).to(device_)\n",
    "        loss = loss_fn(predictions_loss, lbels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #optimizer_.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer_.step()\n",
    "        scheduler_.step()\n",
    "\n",
    "        predictions += predictions_loss\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    return true_labels, predictions, avg_epoch_loss\n",
    "\n",
    "\n",
    "def validation(test_loader, device_, loss_fn):\n",
    "    global model\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        \n",
    "        ##INPUTS:\n",
    "        img_key = 'pixel_values' #'input_ids'\n",
    "        img_input_ids = batch['portraits'][img_key].type(torch.float).to(device_)\n",
    "\n",
    "\n",
    "        with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            outputs = model(img_input_ids=img_input_ids).to(device_)\n",
    "            logits = outputs\n",
    "\n",
    "            predictions += logits.squeeze().detach().cpu().tolist()\n",
    "            predictions_loss = torch.Tensor(logits.squeeze().detach().cpu()).to(device_)\n",
    "\n",
    "            loss = loss_fn(predictions_loss, torch.Tensor(batch['labels'].float()).to(device_))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(test_loader)\n",
    "\n",
    "    return true_labels, predictions, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ca_Naxca import regression_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "##TRAIN THE MODEL.\n",
    "\n",
    "optimizer_ = torch.optim.AdamW(model.parameters(), lr = learning_rate, eps = eps_value)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler_ = get_linear_schedule_with_warmup(optimizer_, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "loss_fn = nn.MSELoss()  # Loss function for regression problems\n",
    "\n",
    "\n",
    "trainval = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    avg_epoch_loss = 0\n",
    "\n",
    "    # Train the model:\n",
    "    true_labels, predictions, train_loss = training_loop(train_dataloader, predictions, true_labels, optimizer_, scheduler_, device, loss_fn)\n",
    "    # Test the model:\n",
    "    valid_labels, valid_predict, val_loss = validation(test_dataloader, device, loss_fn)\n",
    "\n",
    "    # Is it good enough?\n",
    "    reporte = regression_report(valid_labels, valid_predict, [i for i in range(len(valid_labels))])\n",
    "    reps = reporte.display()\n",
    "    print(reps)\n",
    "    reps.to_csv(\".//final_reports//Img//reporte_img.csv\")\n",
    "\n",
    "    for x in [[valid_labels[i], valid_predict[i]] for i in range(10)]:\n",
    "        print(x)\n",
    "\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f \"%(train_loss, val_loss))\n",
    "    print()\n",
    "    trainval.append([train_loss, val_loss])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "jsonfile = open(\".//final_reports//Img//train_val_loss.json\", \"w\")\n",
    "json.dump(trainval, jsonfile)\n",
    "jsonfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
